# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18k7oMXB21tT7votXwpqFibV0UidItlQn
"""

# ============================================================
# Cell 1 — Add garment type labels to wash_labels.csv (Fixed)
# ============================================================

import os
import pandas as pd
from google.colab import drive

drive.mount('/content/drive')

# Paths
WASH_CSV = "/content/drive/MyDrive/wash_ai_project/wash_labels.csv"
ARTICLES_CSV = "/content/drive/MyDrive/hm/articles.csv"
SAVE_PATH = "/content/drive/MyDrive/wash_ai_project/wash_labels_with_garment.csv"

print("wash CSV exists?  ", os.path.exists(WASH_CSV))
print("articles CSV exists?", os.path.exists(ARTICLES_CSV))

# -----------------------------
# 1) Load both CSV files
# -----------------------------
df_wash = pd.read_csv(WASH_CSV)
df_art  = pd.read_csv(ARTICLES_CSV)

print("\nwash_labels columns:")
print(df_wash.columns.tolist())
print("\narticles columns:")
print(df_art.columns.tolist())

# Check required columns
assert "article_id" in df_wash.columns
assert "article_id" in df_art.columns
assert "product_type_name" in df_art.columns

# -----------------------------
# 2) Merge to add garment type
# -----------------------------
df_merge = df_wash.merge(
    df_art[["article_id", "product_type_name"]],
    on="article_id",
    how="left"
)

missing = df_merge["product_type_name"].isna().sum()
print(f"\nRows with missing garment type after merge: {missing}")

# Drop missing garment entries
df_merge = df_merge.dropna(subset=["product_type_name"]).reset_index(drop=True)

# -----------------------------
# 3) Encode garment label
# -----------------------------
unique_garments = sorted(df_merge["product_type_name"].unique())
GARMENT2IDX = {g:i for i, g in enumerate(unique_garments)}

df_merge["garment_type"]  = df_merge["product_type_name"].astype(str)
df_merge["garment_label"] = df_merge["garment_type"].map(GARMENT2IDX).astype(int)

print(f"\nNumber of garment types: {len(unique_garments)}")
print("Sample garment types:", unique_garments[:20])

print("\nGarment label distribution:")
print(df_merge["garment_label"].value_counts().head())

# -----------------------------
# 4) Save the updated CSV
# -----------------------------
df_merge.to_csv(SAVE_PATH, index=False)

print("\nSaved updated CSV to:")
print(SAVE_PATH)

print("\nExample rows:")
print(df_merge[[
    "image_path",
    "article_id",
    "color_group",
    "fabric_group",
    "wash_cycle",
    "garment_type",
    "garment_label"
]].head())

from google.colab import drive
drive.mount("/content/drive")

import os
import pandas as pd

# ------------------------------------------------------
# 1) Paths to your CSV files in Google Drive
# ------------------------------------------------------
BASE_DIR      = "/content/drive/MyDrive"
WASH_CSV      = os.path.join(BASE_DIR, "wash_ai_project", "wash_labels.csv")
ARTICLES_CSV  = os.path.join(BASE_DIR, "hm", "articles.csv")

print("Wash CSV path     :", WASH_CSV)
print("Articles CSV path :", ARTICLES_CSV)
print("wash CSV exists?  ", os.path.exists(WASH_CSV))
print("articles exists?  ", os.path.exists(ARTICLES_CSV))

# ------------------------------------------------------
# 2) Load both CSV files
# ------------------------------------------------------
df_wash = pd.read_csv(WASH_CSV)
df_art  = pd.read_csv(ARTICLES_CSV)

print("\nwash_labels columns:")
print(df_wash.columns.tolist())
print("\narticles columns:")
print(df_art.columns.tolist())

# ------------------------------------------------------
# 3) Merge wash_labels with articles on article_id
#    and create a garment_type column
# ------------------------------------------------------
merge_cols = [
    "article_id",
    "product_type_name",      # fine-grained garment name, e.g. T-shirt, Dress, Bra
    "product_group_name",     # higher-level group, e.g. Garment Upper body, Shoes
    "garment_group_name",     # another grouping, e.g. Jersey Basic, Under-, Nightwear
]

df_merged = df_wash.merge(
    df_art[merge_cols],
    on="article_id",
    how="left"
)

# Rename product_type_name -> garment_type for clarity
df_merged = df_merged.rename(columns={"product_type_name": "garment_type"})

print("\nSample merged rows:")
print(df_merged[["image_path", "article_id", "color_group",
                 "fabric_group", "wash_cycle", "garment_type",
                 "product_group_name", "garment_group_name"]].head())

# ------------------------------------------------------
# 4) Mark accessories / non-clothing items
#    (cap, shoes, hat, bag, sunglasses, etc.)
# ------------------------------------------------------
ACCESSORY_GROUP_KEYWORDS = [
    "Shoe", "Shoes", "Boot", "Sneaker", "Sandal", "Flip flop",
    "Accessory", "Accessories",
    "Bag", "Backpack", "Tote", "Wallet",
    "Jewellery", "Jewelry", "Earring", "Necklace", "Bracelet", "Ring",
    "Sunglass", "Glasses",
    "Hat", "Cap", "Beanie", "Bucket hat",
    "Scarf", "Glove", "Belt",
    "Cosmetic", "Beauty",
    "Home", "Cushion", "Towel", "Blanket", "Bedding",
]

ACCESSORY_TYPE_KEYWORDS = [
    "Cap", "Hat", "Beanie", "Bucket hat",
    "Ballerinas", "Flat shoe", "Flat shoes", "Boot", "Bootie",
    "Sandal", "Sneaker", "Trainer",
    "Bag", "Backpack", "Wallet",
    "Sunglass", "Glasses",
    "Earring", "Necklace", "Bracelet", "Ring",
    "Scarf", "Glove", "Mitten", "Belt",
    "Cushion", "Towel",
    "Fine cosmetics", "Cosmetics",
]

def is_accessory_row(row):
    """Return True if this row is clearly a non-clothing accessory."""
    t = str(row.get("garment_type", "")).lower()
    g = str(row.get("product_group_name", "")).lower()
    gg = str(row.get("garment_group_name", "")).lower()

    # Check specific type names (garment_type)
    if any(k.lower() in t for k in ACCESSORY_TYPE_KEYWORDS):
        return True

    # Check higher-level group names
    if any(k.lower() in g for k in ACCESSORY_GROUP_KEYWORDS):
        return True

    if any(k.lower() in gg for k in ACCESSORY_GROUP_KEYWORDS):
        return True

    return False

df_merged["is_accessory"] = df_merged.apply(is_accessory_row, axis=1)
df_merged["is_clothing"]  = (~df_merged["is_accessory"]).astype(int)

print("\nAccessory vs clothing counts:")
print(df_merged["is_clothing"].value_counts().rename({1: "clothing", 0: "accessory / non-clothing"}))

# ------------------------------------------------------
# 5) Keep ONLY clothing rows for garment-type classification
# ------------------------------------------------------
df_clothes = df_merged[df_merged["is_clothing"] == 1].copy()

# Drop rows with missing garment_type
df_clothes = df_clothes[~df_clothes["garment_type"].isna()].copy()

# Build integer labels for garment_type
garment_types = sorted(df_clothes["garment_type"].unique())
garment2id = {g: i for i, g in enumerate(garment_types)}
df_clothes["garment_label"] = df_clothes["garment_type"].map(garment2id)

print("\nNumber of clothing garment types:", len(garment_types))
print("Some garment types:", garment_types[:20])
print("\nGarment label distribution (top 10):")
print(df_clothes["garment_label"].value_counts().head(10))

# ------------------------------------------------------
# 6) Save:
#    (a) full merged CSV with is_clothing flag
#    (b) filtered clothing-only CSV with garment_label
# ------------------------------------------------------
FULL_OUT_PATH   = os.path.join(BASE_DIR, "wash_ai_project", "wash_labels_with_garment_flag.csv")
CLOTHES_OUT_PATH = os.path.join(BASE_DIR, "wash_ai_project", "wash_labels_clothes_only.csv")

df_merged.to_csv(FULL_OUT_PATH, index=False)
df_clothes.to_csv(CLOTHES_OUT_PATH, index=False)

print("\nSaved full merged CSV (with is_clothing flag) to:")
print(FULL_OUT_PATH)

print("Saved clothing-only CSV (for training garment-type head) to:")
print(CLOTHES_OUT_PATH)

print("\nExample clothing rows:")
print(df_clothes[[
    "image_path", "color_group", "fabric_group",
    "wash_cycle", "garment_type", "garment_label"
]].head())

# =====================================================
# SUPER-FAST COPY — Only copy images used in your CSV
# =====================================================
import os
import shutil
from tqdm.auto import tqdm
import pandas as pd

CSV = "/content/drive/MyDrive/wash_ai_project/wash_labels_clothes_only.csv"
df = pd.read_csv(CSV)

print("CSV loaded:", len(df), "rows")

# Output folder
LOCAL_ROOT = "/content/wash_local_crops"

if not os.path.exists(LOCAL_ROOT):
    os.makedirs(LOCAL_ROOT)

# Count how many need to be copied
unique_paths = df["image_path"].unique()
print("Unique image paths:", len(unique_paths))

copied = 0

for src in tqdm(unique_paths):
    # Make a short filename for local storage
    fname = os.path.basename(src)
    dst = os.path.join(LOCAL_ROOT, fname)

    if not os.path.exists(dst):
        try:
            shutil.copy(src, dst)
            copied += 1
        except:
            pass

print("\nFinished.")
print("Images copied:", copied)
print("Local dataset ready at:", LOCAL_ROOT)

# =====================================================
# Fix CSV paths to use local /content/wash_local_crops
# =====================================================

import pandas as pd

CSV_IN  = "/content/drive/MyDrive/wash_ai_project/wash_labels_clothes_only.csv"
CSV_OUT = "/content/wash_labels_local.csv"

df = pd.read_csv(CSV_IN)

OLD_PREFIX = "/content/drive/MyDrive/wash_ai_outputs/crops_backup/content/wash_ai_project/crops"
NEW_PREFIX = "/content/wash_local_crops"

df["image_path"] = df["image_path"].apply(
    lambda p: p.replace(OLD_PREFIX, NEW_PREFIX)
)

df.to_csv(CSV_OUT, index=False)

print("✔ CSV updated and saved locally:")
print("  ", CSV_OUT)

# Show a preview
print(df.head())

import pandas as pd
import os

# Original CSV on Drive
CSV_ORIG = "/content/drive/MyDrive/wash_ai_project/wash_labels_clothes_only.csv"

df = pd.read_csv(CSV_ORIG)

print("Loaded original CSV:", len(df), "rows")

# Extract filename only
df["filename"] = df["image_path"].apply(lambda p: os.path.basename(p))

# New correct path (local)
df["image_path"] = df["filename"].apply(lambda f: f"/content/wash_local_crops/{f}")

# Remove helper column
df = df.drop(columns=["filename"])

# Save to local correct CSV
LOCAL_CSV = "/content/wash_labels_local.csv"
df.to_csv(LOCAL_CSV, index=False)

print("\n✔ NEW local CSV saved!")
print("Path:", LOCAL_CSV)

# Show 5 rows to confirm
print(df.head())

# ============================================================
# 3-CLASS GARMENT CLASSIFIER (TOP / BOTTOM / ONEPIECE)
# Uses: /content/wash_labels_local.csv
# ============================================================

import os
import time
import pandas as pd
from collections import Counter
from PIL import Image

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import timm
from tqdm.auto import tqdm

CSV_LOCAL = "/content/wash_labels_local.csv"
assert os.path.exists(CSV_LOCAL), f"Local CSV not found: {CSV_LOCAL}"

df = pd.read_csv(CSV_LOCAL)
assert "garment_type" in df.columns, "garment_type column missing"
assert "split" in df.columns, "split column missing"

print("Loaded:", CSV_LOCAL)
print("Rows:", len(df))


# ------------------------------------------------------------
# 1) Map garment_type → 3 macro classes
# ------------------------------------------------------------
def to_3class(gt: str) -> str:
    gt = str(gt).lower().strip()

    # TOP
    if any(k in gt for k in [
        "t-shirt", "tshirt", "shirt", "blouse", "top",
        "vest top", "hoodie", "sweater", "cardigan",
        "polo", "tank", "sweatshirt"
    ]):
        return "TOP"

    # BOTTOM
    if any(k in gt for k in [
        "trousers", "shorts", "skirt", "leggings", "tights", "jeans"
    ]):
        return "BOTTOM"

    # ONEPIECE
    if any(k in gt for k in [
        "dress", "jumpsuit", "playsuit", "dungarees"
    ]):
        return "ONEPIECE"

    return "OTHER"

df["garment_3class"] = df["garment_type"].apply(to_3class)

print("\n3-class label counts (before filtering):")
print(df["garment_3class"].value_counts())

# keep only the 3 classes
keep_classes = ["TOP", "BOTTOM", "ONEPIECE"]
df = df[df["garment_3class"].isin(keep_classes)].reset_index(drop=True)

print("\nAfter filtering to TOP/BOTTOM/ONEPIECE:")
print(df["garment_3class"].value_counts())

CLASS2IDX = {name: i for i, name in enumerate(sorted(keep_classes))}
IDX2CLASS = {i: name for name, i in CLASS2IDX.items()}

df["g3_label"] = df["garment_3class"].map(CLASS2IDX).astype(int)
num_classes = len(CLASS2IDX)

print("\nClass → index mapping:", CLASS2IDX)
print("Number of classes:", num_classes)


# ------------------------------------------------------------
# 2) Train/Val split (reuse split column)
# ------------------------------------------------------------
df_train = df[df["split"] == "train"].reset_index(drop=True)
df_val   = df[df["split"] == "val"].reset_index(drop=True)

print(f"\nTrain samples: {len(df_train)}")
print(f"Val samples:   {len(df_val)}")


# ------------------------------------------------------------
# 3) Dataset + transforms
# ------------------------------------------------------------
IMG_SIZE = 224
MEAN = [0.485, 0.456, 0.406]
STD  = [0.229, 0.224, 0.225]

train_tf = transforms.Compose([
    transforms.Resize(IMG_SIZE + 16),
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2,
                           saturation=0.2, hue=0.04),
    transforms.ToTensor(),
    transforms.Normalize(MEAN, STD),
])

val_tf = transforms.Compose([
    transforms.Resize(IMG_SIZE + 16),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(MEAN, STD),
])

class G3Dataset(Dataset):
    def __init__(self, df, tf):
        self.df = df.reset_index(drop=True)
        self.tf = tf

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row["image_path"]
        img = Image.open(img_path).convert("RGB")
        img = self.tf(img)
        y = int(row["g3_label"])
        return img, torch.tensor(y, dtype=torch.long)

train_ds = G3Dataset(df_train, train_tf)
val_ds   = G3Dataset(df_val,   val_tf)

BATCH_SIZE = 64  # if OOM, reduce to 32

train_loader = DataLoader(
    train_ds,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,   # 0 to avoid Colab multiprocess issues
    pin_memory=True,
)

val_loader = DataLoader(
    val_ds,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0,
    pin_memory=True,
)


# ------------------------------------------------------------
# 4) Model: ConvNeXt-Tiny
# ------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("\nDevice:", device)

class G3ConvNeXt(nn.Module):
    def __init__(self, model_name="convnext_tiny", num_classes=3, dropout=0.2):
        super().__init__()
        self.backbone = timm.create_model(
            model_name,
            pretrained=True,
            num_classes=0,
            global_pool="avg",
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.backbone.num_features, num_classes)

    def forward(self, x):
        feat = self.backbone(x)
        feat = self.dropout(feat)
        return self.fc(feat)

model = G3ConvNeXt(num_classes=num_classes).to(device)


# ------------------------------------------------------------
# 5) Loss with class weights
# ------------------------------------------------------------
counts = Counter(df_train["g3_label"].tolist())
total_train = len(df_train)
weights = []
for c in range(num_classes):
    freq = counts.get(c, 1)
    weights.append(total_train / (num_classes * freq))

class_weights = torch.tensor(weights, dtype=torch.float32, device=device)
print("\nClass weights:", class_weights.cpu().numpy())

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)


# ------------------------------------------------------------
# 6) Training loop
# ------------------------------------------------------------
def run_epoch(loader, training=True):
    if training:
        model.train()
    else:
        model.eval()

    total_loss = 0.0
    correct = 0
    total = 0

    for imgs, labels in tqdm(loader, leave=False):
        imgs   = imgs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        if training:
            optimizer.zero_grad()

        with torch.set_grad_enabled(training):
            logits = model(imgs)
            loss = criterion(logits, labels)
            if training:
                loss.backward()
                optimizer.step()

        preds = logits.argmax(dim=1)
        bs = labels.size(0)

        total_loss += loss.item() * bs
        correct    += (preds == labels).sum().item()
        total      += bs

    avg_loss = total_loss / max(1, total)
    acc = correct / max(1, total)
    return avg_loss, acc


EPOCHS = 10
best_val_acc = 0.0
SAVE_PATH = "/content/best_garment_3class.pt"

print("\n==========================")
print(" 3-CLASS TRAINING STARTED ")
print("==========================\n")

for epoch in range(1, EPOCHS + 1):
    t0 = time.time()

    train_loss, train_acc = run_epoch(train_loader, training=True)
    val_loss,   val_acc   = run_epoch(val_loader,   training=False)
    scheduler.step()

    dt = (time.time() - t0) / 60.0

    print(
        f"[Epoch {epoch:02d}] "
        f"TrainLoss={train_loss:.4f} | TrainAcc={train_acc*100:5.2f}% || "
        f"ValLoss={val_loss:.4f}   | ValAcc={val_acc*100:5.2f}% | "
        f"Time={dt:.1f} min"
    )

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), SAVE_PATH)
        print(f"  ✓ NEW BEST — ValAcc = {val_acc*100:5.2f}%")
        print(f"    Saved to: {SAVE_PATH}")

print("\n==========================")
print(" 3-CLASS TRAINING COMPLETE")
print("==========================")
print(f"Best Validation Accuracy: {best_val_acc*100:.2f}%")
print("Best model saved at:", SAVE_PATH)
print("3-class mapping (idx → name):", IDX2CLASS)

# ===============================================================
# VISUALIZATION CELL (compact CM, colorful bars, only true images)
# ===============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from math import ceil
from PIL import Image
import torch

model.eval()

# -----------------------------------------
# 1) Get validation dataframe & class names
# -----------------------------------------
df_val = val_loader.dataset.df.copy()

unique_labels = sorted(df_val["garment_label"].unique())
num_classes = len(unique_labels)

label_to_name = (
    df_val.groupby("garment_label")["garment_type"]
    .first()
    .to_dict()
)

class_names = [label_to_name[l] for l in unique_labels]

print("Total classes:", num_classes)
print("Example class names:", class_names[:10])
print("Unique label ids:", unique_labels)

# -----------------------------------------
# 2) Collect predictions + confidence
# -----------------------------------------
all_true, all_pred, all_conf = [], [], []

with torch.no_grad():
    for batch in val_loader:
        imgs   = batch[0].to(device)
        labels = batch[1].to(device)

        logits = model(imgs)
        probs  = torch.softmax(logits, dim=1)
        conf, pred = probs.max(dim=1)

        all_true.append(labels.cpu().numpy())
        all_pred.append(pred.cpu().numpy())
        all_conf.append(conf.cpu().numpy())

all_true = np.concatenate(all_true)
all_pred = np.concatenate(all_pred)
all_conf = np.concatenate(all_conf)

# map raw labels (0..44 but maybe with gaps) to 0..num_classes-1
true_mapped = np.array([unique_labels.index(t) for t in all_true])
pred_mapped = np.array([unique_labels.index(p) for p in all_pred])

# =====================================================
# 3) Confusion matrix (Top-K) with percentage text
# =====================================================
cm = confusion_matrix(true_mapped, pred_mapped, labels=np.arange(num_classes))
class_counts = cm.sum(axis=1, keepdims=True)

cm_pct = np.zeros_like(cm, dtype=float)
np.divide(cm, class_counts, out=cm_pct, where=class_counts != 0)
cm_pct *= 100.0

TOP_K = 10
freq = class_counts.ravel()
top_idx = np.argsort(freq)[::-1][:TOP_K]
top_idx = np.sort(top_idx)

cm_top = cm_pct[np.ix_(top_idx, top_idx)]
names_top = [class_names[i] for i in top_idx]

fig, ax = plt.subplots(figsize=(6, 5))
im = ax.imshow(cm_top, cmap="Blues", vmin=0, vmax=100)

ax.set_xticks(np.arange(len(names_top)))
ax.set_yticks(np.arange(len(names_top)))
ax.set_xticklabels(names_top, rotation=45, ha="right", fontsize=8)
ax.set_yticklabels(names_top, fontsize=8)
ax.set_xlabel("Predicted")
ax.set_ylabel("True")
ax.set_title(f"Confusion Matrix (Top {TOP_K} Classes, % per True Label)")

# put text in *every* square
for i in range(cm_top.shape[0]):
    for j in range(cm_top.shape[1]):
        val = cm_top[i, j]
        if val < 0.05 and cm_top[i, j] == 0:
            txt = "<0.1%"
        else:
            txt = f"{val:.1f}%"
        ax.text(
            j, i, txt,
            ha="center", va="center",
            fontsize=7,
            color="white" if val > 50 else "black"
        )

cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
cbar.set_label("Percent per True Class", fontsize=9)

plt.tight_layout()
plt.show()

# =====================================================
# 4) Per-class accuracy bar chart (colorful, sorted)
# =====================================================
class_acc = np.zeros(num_classes, dtype=float)
for i in range(num_classes):
    if class_counts[i, 0] > 0:
        class_acc[i] = cm[i, i] / class_counts[i, 0]

order = np.argsort(class_acc)[::-1]
acc_sorted = class_acc[order] * 100.0
names_sorted = [class_names[i] for i in order]

fig, ax = plt.subplots(figsize=(10, 5))

cmap = plt.cm.get_cmap("tab20", num_classes)
colors = [cmap(i) for i in range(num_classes)]

bars = ax.bar(np.arange(num_classes), acc_sorted,
              color=[colors[i] for i in order])

ax.set_xticks(np.arange(num_classes))
ax.set_xticklabels(names_sorted, rotation=60, ha="right", fontsize=7)
ax.set_ylabel("Accuracy (%)")
ax.set_title("Per-Class Validation Accuracy (Sorted)")

for rect, v in zip(bars, acc_sorted):
    ax.text(rect.get_x() + rect.get_width()/2, v + 1,
            f"{v:.1f}%", ha="center", fontsize=6)

plt.tight_layout()
plt.show()

# =====================================================
# 5) Show ONLY correct predictions (images)
# =====================================================
indices = np.arange(len(all_true))
correct_mask = (all_true == all_pred)

def show_correct(mask, title, n=12):
    idxs = indices[mask]
    if len(idxs) == 0:
        print("No correct samples!")
        return

    n = min(n, len(idxs))
    idxs = np.random.choice(idxs, size=n, replace=False)
    rows = ceil(n / 4)

    fig, axes = plt.subplots(rows, 4, figsize=(12, rows * 3))
    axes = axes.ravel()

    for ax, idx in zip(axes, idxs):
        img_path = df_val.iloc[idx]["image_path"]
        img = Image.open(img_path).convert("RGB")

        t = all_true[idx]
        c = all_conf[idx] * 100

        ax.imshow(img)
        ax.axis("off")
        ax.set_title(f"{label_to_name[t]} ({c:.1f}%)", fontsize=8)

    for ax in axes[len(idxs):]:
        ax.axis("off")

    fig.suptitle(title, fontsize=12)
    plt.tight_layout()
    plt.show()

show_correct(correct_mask, "Correct Predictions (Random)")

# ============================================
# SAVE & DOWNLOAD ALL IMPORTANT PROJECT FILES
# ============================================

import os
import zipfile
from datetime import datetime
from google.colab import files

# Root folder of your project on Google Drive
PROJECT_ROOT = "/content/drive/MyDrive/wash_ai_project"

# List of files (relative to PROJECT_ROOT) that we want to back up
FILES_TO_SAVE = [
    "wash_labels.csv",
    "wash_labels_with_garment.csv",
    "wash_labels_with_garment_flag.csv",
    "wash_labels_clothes_only.csv",
    "articles.csv",
    "best_model_wash.pt",
    "best_garment_model.pt",
    "ai.jpg",
    "app.py",
]

print("=== Checking files to include in backup ===")
existing_files = []
for fname in FILES_TO_SAVE:
    full_path = os.path.join(PROJECT_ROOT, fname)
    if os.path.exists(full_path):
        existing_files.append(full_path)
        print(f"  ✓ Found:   {fname}")
    else:
        print(f"  ✗ Missing: {fname}")

if not existing_files:
    print("\nNo files found to save. Nothing to back up.")
else:
    print(f"\nTotal files that will be added to ZIP: {len(existing_files)}")

    # Create a timestamped ZIP filename so you can keep multiple backups
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"wash_ai_backup_{timestamp}.zip"
    ZIP_PATH = os.path.join("/content", zip_name)

    # Create ZIP archive
    with zipfile.ZipFile(ZIP_PATH, "w", compression=zipfile.ZIP_DEFLATED) as zipf:
        for f in existing_files:
            arcname = os.path.basename(f)  # store only the file name inside the zip
            zipf.write(f, arcname)
            print(f"  → Added to ZIP: {arcname}")

    print("\nZIP archive created at:", ZIP_PATH)
    print("Starting download...")

    # Trigger download to your local machine
    files.download(ZIP_PATH)

